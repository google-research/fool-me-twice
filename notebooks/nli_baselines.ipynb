{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FoolMeTwice NLI Baselines",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPZfQJpvIe1g"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/google-research/fool-me-twice/blob/master/notebooks/nli_baselines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCU_BBmxItFu"
      },
      "source": [
        "##### Copyright 2021 The Google AI Language Team Authors\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8i8Cu-GVIcDn"
      },
      "source": [
        "# Copyright 2021 The Google AI Language Team Authors.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qN6thwTnLNN0"
      },
      "source": [
        "## Set-Up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abHV1zyjr9Km"
      },
      "source": [
        "!pip install tf-models-official\n",
        "!pip install tensorflow-text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2USAOydP2lGX"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_text as text\n",
        "\n",
        "from official.nlp.modeling import networks\n",
        "from official.nlp.modeling import models\n",
        "from official.nlp.bert import configs\n",
        "\n",
        "import json\n",
        "import requests"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GhAr-_xegD0"
      },
      "source": [
        "Setting up the TPU (make sure you use a TPU runtime).  This needs to run before doing anything else to avoid issues."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGwUrdWkY3X5"
      },
      "source": [
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "strategy = tf.distribute.TPUStrategy(resolver)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3QCbm0PTmLu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "9fe25dd7-ef28-4840-ce10-19c2d069a77f"
      },
      "source": [
        "#@title Load JSONL Data\n",
        "\n",
        "LABELS = {'SUPPORTS': '0', 'REFUTES': '1'}\n",
        "\n",
        "def json_to_dataset(url_path):\n",
        "  def _load_json():\n",
        "    data_tuples = []\n",
        "\n",
        "    data = requests.get(url_path)\n",
        "\n",
        "    for line in data.content.decode('utf-8').split('\\n'):\n",
        "      if not line:\n",
        "        continue\n",
        "      json_line = json.loads(line)\n",
        "      # To work with CloudTPU, we cannot use a generator but have to create the\n",
        "      # data using `from_tensor_slices`.  Because this requires a homogenous \n",
        "      # tuples, the label is a string at this point, and we only parse it to \n",
        "      # int later.\n",
        "      data_tuples.append(\n",
        "          (json_line['text'],  ' '.join(\n",
        "              x['text'] for x in json_line['gold_evidence']),\n",
        "              LABELS[json_line['label']]))\n",
        "\n",
        "    return data_tuples\n",
        "\n",
        "  data_tuples = _load_json()\n",
        "  print(f'Loaded {len(data_tuples)} examples from {url_path}')\n",
        "\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(data_tuples).map(\n",
        "      lambda x: {\n",
        "          'hypothesis': x[0],\n",
        "          'premise': x[1],\n",
        "          'label': tf.strings.to_number(x[2], tf.int32),\n",
        "      })\n",
        "  \n",
        "  # Hand-holding the dataset to know its own size.  This will make Keras'  \n",
        "  # logging more informative, as it will know how many batches to expect per\n",
        "  # epoch.\n",
        "  dataset = dataset.apply(tf.data.experimental.assert_cardinality(len(data_tuples)))\n",
        "  return dataset\n",
        "\n",
        "fm2_train_dataset = json_to_dataset('https://raw.githubusercontent.com/google-research/fool-me-twice/main/dataset/train.jsonl')\n",
        "fm2_dev_dataset = json_to_dataset('https://raw.githubusercontent.com/google-research/fool-me-twice/main/dataset/dev.jsonl')\n",
        "\n",
        "fever_train_dataset = json_to_dataset('https://storage.googleapis.com/fool-me-twice-media/data/fever/train.jsonl')\n",
        "fever_dev_dataset = json_to_dataset('https://storage.googleapis.com/fool-me-twice-media/data/fever/dev.jsonl')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded 10419 examples from https://raw.githubusercontent.com/google-research/fool-me-twice/main/dataset/train.jsonl\n",
            "Loaded 1169 examples from https://raw.githubusercontent.com/google-research/fool-me-twice/main/dataset/dev.jsonl\n",
            "Loaded 109810 examples from https://storage.googleapis.com/fool-me-twice-media/data/fever/train.jsonl\n",
            "Loaded 13332 examples from https://storage.googleapis.com/fool-me-twice-media/data/fever/dev.jsonl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJRR49qLeW4L"
      },
      "source": [
        "We use tf-text to map over these \"plain text\" examples on the fly, turning them into the format expected by a BERT-based classifier.\n",
        "\n",
        "We consider three \"modes\",\n",
        "\n",
        "- *normal*, i.e. ```[CLS] claim [SEP] evidences [SEP]```\n",
        "- *evidence_only*, i.e. ```[CLS] evidences [SEP]```\n",
        "- *hypothesis_only*, i.e. ```[CLS] claim [SEP]```\n",
        "\n",
        "*normal* corresponds to the proper setting, whereas *evidence_only*  and *hypothesis_only* are diagnostic settings to quantify the amount of artefacts a simple BERT-based classifier (and similar models) could exploit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fk3fEEi_51Q0",
        "cellView": "form"
      },
      "source": [
        "#@title Model Definition\n",
        "\n",
        "NUM_CLASSES = 2  # SUPPORTS, REFUTES\n",
        "MAX_SEQ_LENGTH = 512  #@param {type:\"integer\"}\n",
        "BATCH_SIZE = 32  #@param {type:\"integer\"}\n",
        "CHECKPOINT_PATH = 'gs://cloud-tpu-checkpoints/bert/keras_bert/uncased_L-12_H-768_A-12/bert_model.ckpt'  #@param {type:\"string\"}\n",
        "VOCAB_PATH = 'gs://cloud-tpu-checkpoints/bert/keras_bert/uncased_L-12_H-768_A-12/vocab.txt'  #@param {type:\"string\"}\n",
        "CONFIG_PATH = 'gs://cloud-tpu-checkpoints/bert/keras_bert/uncased_L-12_H-768_A-12/bert_config.json'  #@param {type:\"string\"}\n",
        "\n",
        "def combine_segments(segments, start_of_sequence_id, end_of_segment_id):\n",
        "  \"\"\"Combine one or more input segments for a model's input sequence.\n",
        "\n",
        "  `combine_segments` combines the tokens of one or more input segments to a\n",
        "  single sequence of token values and generates matching segment ids.\n",
        "  `combine_segments` can follow a `Trimmer`, who limit segment lengths and\n",
        "  emit `RaggedTensor` outputs, and can be followed up by `ModelInputPacker`.\n",
        "\n",
        "  See `Detailed Experimental Setup` in `BERT: Pre-training of Deep Bidirectional\n",
        "  Transformers for Language Understanding`\n",
        "  (https://arxiv.org/pdf/1810.04805.pdf) for more examples of combined\n",
        "  segments.\n",
        "\n",
        "\n",
        "  `combine_segments` first flattens and combines a list of one or more\n",
        "  segments\n",
        "  (`RaggedTensor`s of n dimensions) together along the 1st axis, then packages\n",
        "  any special tokens  into a final n dimensional `RaggedTensor`.\n",
        "\n",
        "  And finally `combine_segments` generates another `RaggedTensor` (with the\n",
        "  same rank as the final combined `RaggedTensor`) that contains a distinct int\n",
        "  id for each segment.\n",
        "\n",
        "  Example usage:\n",
        "\n",
        "  ```\n",
        "  segment_a = [[1, 2],\n",
        "               [3, 4,],\n",
        "               [5, 6, 7, 8, 9]]\n",
        "\n",
        "  segment_b = [[10, 20,],\n",
        "               [30, 40, 50, 60,],\n",
        "               [70, 80]]\n",
        "  expected_combined, expected_ids = combine_segments([segment_a, segment_b])\n",
        "\n",
        "  # segment_a and segment_b have been combined w/ special tokens describing\n",
        "  # the beginning of a sequence and end of a sequence inserted.\n",
        "  expected_combined=[\n",
        "   [101, 1, 2, 102, 10, 20, 102],\n",
        "   [101, 3, 4, 102, 30, 40, 50, 60, 102],\n",
        "   [101, 5, 6, 7, 8, 9, 102, 70, 80, 102],\n",
        "  ]\n",
        "\n",
        "  # ids describing which items belong to which segment.\n",
        "  expected_ids=[\n",
        "   [0, 0, 0, 0, 1, 1, 1],\n",
        "   [0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
        "   [0, 0, 0, 0, 0, 0, 0, 1, 1, 1]]\n",
        "  ```\n",
        "\n",
        "  Args:\n",
        "    segments: A list of `RaggedTensor`s with the tokens of the input segments.\n",
        "      All elements must have the same dtype (int32 or int64), same rank, and\n",
        "      same dimension 0 (namely batch size). Slice `segments[i][j, ...]`\n",
        "      contains the tokens of the i-th input segment to the j-th example in the\n",
        "      batch.\n",
        "    start_of_sequence_id: a python int or scalar Tensor containing the id used\n",
        "      to denote the start of a sequence (e.g. `[CLS]` token in BERT\n",
        "      terminology).\n",
        "    end_of_segment_id: a python int or scalar Tensor containing the id used to\n",
        "      denote end of a segment (e.g. the `[SEP]` token in BERT terminology).\n",
        "\n",
        "  Returns:\n",
        "    a tuple of (combined_segments, segment_ids), where:\n",
        "\n",
        "    combined_segments: A `RaggedTensor` with segments combined and special\n",
        "      tokens inserted.\n",
        "    segment_ids:  A `RaggedTensor` w/ the same shape as `combined_segments`\n",
        "      and containing int ids for each item detailing the segment that they\n",
        "      correspond to.\n",
        "  \"\"\"\n",
        "  start_of_sequence_id = tf.convert_to_tensor(\n",
        "      start_of_sequence_id, dtype=tf.int64)\n",
        "  end_of_segment_id = tf.convert_to_tensor(\n",
        "      end_of_segment_id, dtype=tf.int64)\n",
        "\n",
        "  # Create special tokens ([CLS] and [SEP]) that will be combined with the\n",
        "  # segments\n",
        "  if len(segments) <= 0:\n",
        "    raise ValueError(\"`segments` must be a nonempty list.\")\n",
        "  segment_dtype = segments[0].dtype\n",
        "  if segment_dtype not in (tf.int32, tf.int64):\n",
        "    raise ValueError(\"`segments` must have elements with dtype of int32 or \" +\n",
        "                     \"int64\")\n",
        "  start_sequence_id = tf.cast(start_of_sequence_id, segment_dtype)\n",
        "  end_segment_id = tf.cast(end_of_segment_id, segment_dtype)\n",
        "  start_seq_tokens = tf.tile([start_sequence_id], [segments[0].nrows()])\n",
        "  end_segment_tokens = tf.tile([end_segment_id], [segments[0].nrows()])\n",
        "  for i in range(segments[0].ragged_rank):\n",
        "    start_seq_tokens = tf.expand_dims(start_seq_tokens, 1)\n",
        "    end_segment_tokens = tf.expand_dims(end_segment_tokens, 1)\n",
        "  special_token_segment_template = tf.ones_like(start_seq_tokens)\n",
        "\n",
        "  # Combine all segments w/ special tokens\n",
        "  segments_to_combine = [start_seq_tokens]\n",
        "  for seg in segments:\n",
        "    segments_to_combine.append(seg)\n",
        "    segments_to_combine.append(end_segment_tokens)\n",
        "  segments_combined = tf.concat(segments_to_combine, 1)\n",
        "\n",
        "  # Create the segment ids, making sure to account for special tokens.\n",
        "  segment_ids_to_combine = []\n",
        "  segment_ids_to_combine.append(special_token_segment_template * 0)\n",
        "  for i, item in enumerate(segments):\n",
        "    # Add segment id\n",
        "    segment_id = tf.ones_like(item) * i\n",
        "    segment_ids_to_combine.append(segment_id)\n",
        "\n",
        "    # Add for SEP\n",
        "    special_token_segment_id = special_token_segment_template * i\n",
        "    segment_ids_to_combine.append(special_token_segment_id)\n",
        "\n",
        "  segment_ids = tf.concat(segment_ids_to_combine, 1)\n",
        "  return segments_combined, segment_ids\n",
        "\n",
        "# Sets up the BERT tokenizer using tf-text.\n",
        "vocab_table = tf.lookup.StaticVocabularyTable(\n",
        "        tf.lookup.TextFileInitializer(\n",
        "            filename=VOCAB_PATH,\n",
        "            key_dtype=tf.string,\n",
        "            key_index=tf.lookup.TextFileIndex.WHOLE_LINE,\n",
        "            value_dtype=tf.int64,\n",
        "            value_index=tf.lookup.TextFileIndex.LINE_NUMBER\n",
        "        ), \n",
        "        num_oov_buckets=1)\n",
        "cls_id, sep_id = vocab_table.lookup(tf.convert_to_tensor(['[CLS]', '[SEP]']))\n",
        "bert_tokenizer = text.BertTokenizer(vocab_lookup_table=vocab_table, \n",
        "                                    token_out_type=tf.int64, \n",
        "                                    preserve_unused_token=True, \n",
        "                                    lower_case=True)\n",
        "\n",
        "# The three different settings.\n",
        "def _normal(inputs):\n",
        "  # <string>[batch_size, hyp_length]\n",
        "  hypothesis = inputs['hypothesis']\n",
        "  # <string>[batch_size, prem_length]\n",
        "  premise = inputs['premise']\n",
        "\n",
        "  # <int>[batch_size, hyp_length, (subwords)]\n",
        "  tokenized_hypothesis = bert_tokenizer.tokenize(hypothesis)\n",
        "  # <int>[batch_size, prem_length, (subwords)]\n",
        "  tokenized_premise = bert_tokenizer.tokenize(premise)\n",
        "\n",
        "  # Get rid of the subword dimensions.\n",
        "  # <int>[batch_size, hyp_length]\n",
        "  flat_tokenized_hypothesis = tokenized_hypothesis.merge_dims(1, 2)\n",
        "  flat_tokenized_premise = tokenized_premise.merge_dims(1, 2)\n",
        "\n",
        "  return (flat_tokenized_premise, flat_tokenized_hypothesis)\n",
        "\n",
        "def _evidence_only(inputs):\n",
        "  return (_normal(inputs)[0],)\n",
        "\n",
        "def _hypothesis_only(inputs):\n",
        "  return (_normal(inputs)[1],)\n",
        "\n",
        "def input_pipeline(dataset, mode, shuffle):\n",
        "  \"\"\"Maps the `plain` examples in `dataset` to classifier inputs.\n",
        "\n",
        "  Args:\n",
        "    dataset:  A tf.data.Dataset yielding (unbatched) examples of the form\n",
        "      {'hypothesis': string, 'premise': string, 'label': int}\n",
        "    mode:  One of `normal`, `evidence_only`, or `hypothesis_only`, see above.\n",
        "    shuffle:  Whether or not the dataset gets shuffled.\n",
        "\n",
        "  Returns:\n",
        "    A batched and shuffled dataset yielding (classifier input, label) tuples.\n",
        "  \"\"\"\n",
        "  segment_fn = {\n",
        "      'normal': _normal,\n",
        "      'evidence_only': _evidence_only,\n",
        "      'hypothesis_only': _hypothesis_only,\n",
        "    }[mode]\n",
        "\n",
        "  def to_example(inputs):\n",
        "    segments = segment_fn(inputs)\n",
        "\n",
        "    # BERT input encoding.\n",
        "    # input_ids: <int>[batch_size, hyp_length + prem_length + 3]\n",
        "    # segment_ids: <int>[batch_size, hyp_length + prem_length + 3]\n",
        "    input_ids, segment_ids = combine_segments(\n",
        "        segments=segments, \n",
        "        start_of_sequence_id=cls_id, \n",
        "        end_of_segment_id=sep_id)\n",
        "    \n",
        "    # [batch_size, max_seq_length]\n",
        "    padded_input_ids = input_ids.to_tensor(shape=(None, MAX_SEQ_LENGTH))\n",
        "    padded_segment_ids = segment_ids.to_tensor(shape=(None, MAX_SEQ_LENGTH))\n",
        "    input_mask = tf.cast(padded_input_ids != 0, tf.int64)\n",
        "    \n",
        "    \n",
        "\n",
        "    return ({\n",
        "        'input_word_ids': padded_input_ids, \n",
        "        'input_type_ids': padded_segment_ids,\n",
        "        'input_mask': input_mask,\n",
        "    }, tf.ensure_shape(inputs['label'], (None, )))\n",
        "\n",
        "  dataset = dataset.batch(BATCH_SIZE).map(to_example)\n",
        "  if shuffle:\n",
        "    dataset = dataset.shuffle(1000)\n",
        " \n",
        "  return dataset\n",
        "\n",
        "\n",
        "cfg = configs.BertConfig.from_json_file(json_file=CONFIG_PATH)\n",
        "\n",
        "with strategy.scope():\n",
        "  bert_encoder = networks.TransformerEncoder(\n",
        "        vocab_size=cfg.vocab_size,\n",
        "        hidden_size=cfg.hidden_size,\n",
        "        num_layers=cfg.num_hidden_layers,\n",
        "        num_attention_heads=cfg.num_attention_heads,\n",
        "        max_sequence_length=MAX_SEQ_LENGTH,\n",
        "        type_vocab_size=cfg.type_vocab_size,\n",
        "        intermediate_size=cfg.intermediate_size,\n",
        "        initializer=tf.keras.initializers.TruncatedNormal(\n",
        "            stddev=cfg.initializer_range))\n",
        "  bert_checkpoint = tf.train.Checkpoint(model=bert_encoder)\n",
        "  classifier = models.BertClassifier(\n",
        "      network=bert_encoder,\n",
        "      num_classes=NUM_CLASSES)\n",
        "  \n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)  \n",
        "  \n",
        "  classifier.compile(optimizer=optimizer, \n",
        "                    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
        "                    metrics=tf.keras.metrics.SparseCategoricalAccuracy())     "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWBKpyCrhrRm"
      },
      "source": [
        "Run experiments you want.  The \"mode\" can be `normal` , `evidence_only`, and `hypothesis_only`, see above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPolFWBJhqPf"
      },
      "source": [
        "def run_experiment(training_data: tf.data.Dataset,\n",
        "                   dev_data: tf.data.Dataset,\n",
        "                   mode: str,\n",
        "                   num_epochs: int):\n",
        "  # Reset the BERT model.\n",
        "  with strategy.scope():\n",
        "    bert_checkpoint.restore(CHECKPOINT_PATH).assert_consumed().run_restore_ops()\n",
        "\n",
        "  train_dataset = input_pipeline(dataset=training_data, mode=mode, shuffle=True)\n",
        "  dev_dataset = input_pipeline(dataset=dev_data, mode=mode, shuffle=False)\n",
        "  classifier.fit(x=train_dataset, validation_data=dev_dataset, epochs=num_epochs)\n",
        "\n",
        "def predict(data: tf.data.Dataset):\n",
        "  predictions = classifier.predict(input_pipeline(dataset=data, \n",
        "                                                  mode='normal', \n",
        "                                                  shuffle=False))\n",
        "  results = []\n",
        "  for (example, prediction) in zip(data, predictions):\n",
        "    results.append({\n",
        "        'hypothesis': example['hypothesis'].numpy(),\n",
        "        'premise': example['premise'].numpy(),\n",
        "        'gold_label': example['label'].numpy(),\n",
        "        'predictions': prediction,\n",
        "    })\n",
        "  return results"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXrmg7Ka8taw"
      },
      "source": [
        "# Model Training\n",
        "\n",
        "Train on FM2, eval on FM2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GU6YIro8zUu"
      },
      "source": [
        "run_experiment(training_data=fm2_train_dataset,\n",
        "               dev_data=fm2_dev_dataset,\n",
        "               mode='normal',\n",
        "               num_epochs=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqweU-YsjG4S"
      },
      "source": [
        "Train on FEVER, eval on FEVER."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTubjRfTjI-Q"
      },
      "source": [
        "run_experiment(training_data=fever_train_dataset,\n",
        "               dev_data=fever_dev_dataset,\n",
        "               mode='normal',\n",
        "               num_epochs=10)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}